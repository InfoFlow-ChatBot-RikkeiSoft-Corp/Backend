from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document
from langchain_community.docstore.in_memory import InMemoryDocstore
import os


class VectorDBManager:
    def __init__(self):
        self.submitted_docs = []  # ì œì¶œëœ Docs ê°ì²´ ëª©ë¡
        self.embedding_model = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        self.vectorstore_path = "faiss_index"

        # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (ì—†ìœ¼ë©´ ë¹ˆ DB ìƒì„±)
        if os.path.exists(self.vectorstore_path):
            try:
                self.vectorstore = FAISS.load_local(
                    self.vectorstore_path,
                    self.embedding_model,
                    allow_dangerous_deserialization=True  # ë³´ì•ˆ ì„¤ì • ì¶”ê°€
                )
                print("âœ… ê¸°ì¡´ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # ë¹ˆ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
                self.vectorstore = FAISS(FAISS.build_index(), self.embedding_model)
        else:
            self.vectorstore = FAISS(FAISS.build_index(), self.embedding_model)
            print("ğŸ”„ ìƒˆ ë¹ˆ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    def add_doc_to_db(self, doc):
        """Docs ê°ì²´ë¥¼ ë²¡í„° DBì— ì¶”ê°€"""
        try:
            # í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            splits = text_splitter.split_text(doc.content)

            if not splits:
                raise RuntimeError("Text splitting failed. No valid chunks generated.")
            
            # Document ê°ì²´ ìƒì„± (ë³¸ë¬¸ê³¼ ë©”íƒ€ë°ì´í„° í¬í•¨)
            documents = [
                Document(page_content=split, metadata={"title": doc.title, "url": doc.url})
                for split in splits
            ]

            # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ìƒˆ ë¬¸ì„œ ì¶”ê°€
            self.vectorstore.add_documents(documents)
            print(f"âœ… '{doc.title}' ë¬¸ì„œê°€ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
            self.vectorstore.save_local(self.vectorstore_path)
            self.vectorstore = FAISS.load_local(
                    self.vectorstore_path,
                    self.embedding_model,
                    allow_dangerous_deserialization=True  # ë³´ì•ˆ ì„¤ì • ì¶”ê°€
                )
            print(f"âœ… '{doc.title}' ë¬¸ì„œê°€ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ ë° ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ì œì¶œëœ ë¬¸ì„œ ì €ì¥
            self.submitted_docs.append(doc)

            # ìƒìœ„ 3ê°œ ì²­í¬ ì •ë³´
            vector_details = []
            for i in range(min(3, len(documents))):
                vector_details.append({
                    "vector_index": i + 1,
                    "embedding_excerpt": self.vectorstore.index.reconstruct(i)[:5],  # ì„ë² ë”© ì¼ë¶€ ì¶œë ¥
                    "content_excerpt": documents[i].page_content[:300],  # ì²­í¬ ë³¸ë¬¸ ì¼ë¶€ ì¶œë ¥
                    "title": documents[i].metadata["title"],  # ë¬¸ì„œ ì œëª© ì¶”ê°€
                    "url": documents[i].metadata["url"],  # ë¬¸ì„œ URL ì¶”ê°€
                })

            return vector_details
        except Exception as e:
            raise RuntimeError(f"Error processing document: {e}")

    def get_submitted_docs(self):
        """ì œì¶œëœ ëª¨ë“  ë¬¸ì„œ ë°˜í™˜"""
        return self.submitted_docs
    def get_all_docs_metadata(self):
        """ë²¡í„° DBì— ì €ì¥ëœ ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°(title, url)ë¥¼ ë°˜í™˜"""
        if not self.vectorstore:
            print("âŒ FAISS ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        metadata_list = []
        for doc_id, doc in self.vectorstore.docstore._dict.items():  # docstore ë‚´ë¶€ dict ì ‘ê·¼
            if doc is None:
                continue
            title = doc.metadata.get("title", "ì œëª© ì—†ìŒ")
            url = doc.metadata.get("url", "URL ì—†ìŒ")
            metadata_list.append({"title": title, "url": url})

        return metadata_list