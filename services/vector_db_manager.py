from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_openai import OpenAI, OpenAIEmbeddings
import os

class VectorDBManager:
    def __init__(self, openai_api_key, google_api_key):
        self.submitted_docs = [] 
        self.vectorstore = None
        self.embedding_model = None
        self.vectorstore_path = "faiss_db"
        
        # Initialize embedding model based on the available API key
        if google_api_key:
            os.environ["GOOGLE_API_KEY"] = google_api_key
            self.embedding_model = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        elif openai_api_key:
            self.embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)
        else:
            raise ValueError("Either google_api_key or openai_api_key must be provided.")
            
        # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (ì—†ìœ¼ë©´ ë¹ˆ DB ìƒì„±)
        if os.path.exists(self.vectorstore_path):
            try:
                self.vectorstore = FAISS.load_local(
                    self.vectorstore_path, 
                    self.embedding_model,
                    allow_dangerous_deserialization=True
                    )
                print("âœ… ê¸°ì¡´ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}. ìƒˆ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
                self.initialize_empty_vectorstore()
        else:
            print("ğŸ”„ ìƒˆ ë¹ˆ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            self.initialize_empty_vectorstore()

    def initialize_empty_vectorstore(self):
        """ë¹ˆ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” (ê¸°ë³¸ ë¬¸ì„œ ì¶”ê°€)"""
        default_doc = Document(page_content="This is a default document.", metadata={"title": "Default"})
        self.vectorstore = FAISS.from_documents([default_doc], embedding=self.embedding_model)
        self.vectorstore.save_local(self.vectorstore_path)
        print("âœ… ê¸°ë³¸ ë¬¸ì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

    def generate_embedding(self, text):
        """generate text embedding"""
        if not self.embedding_model:
            raise ValueError("Embedding model is not initialized.")
        return self.embedding_model.embed_query(text)

    def add_doc_to_db(self, doc):
        try:
            print(f"Processing document: {doc.metadata.get('title', 'ì œëª© ì—†ìŒ')}")
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            splits = text_splitter.split_text(doc.content)

            if not splits:
                raise RuntimeError("Text splitting failed. No valid chunks generated.")
            
            # Document ê°ì²´ ìƒì„± (ë³¸ë¬¸ê³¼ ë©”íƒ€ë°ì´í„° í¬í•¨)
            documents = [
                Document(page_content=split, metadata={"title": doc.title, "url": doc.url})
                for split in splits
            ]

            # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ìƒˆ ë¬¸ì„œ ì¶”ê°€
            self.vectorstore.add_documents(documents)
            print(f"âœ… '{doc.title}' ë¬¸ì„œê°€ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
            self.vectorstore.save_local(self.vectorstore_path)
            self.vectorstore = FAISS.load_local(
                    self.vectorstore_path,
                    self.embedding_model,
                    allow_dangerous_deserialization=True  # ë³´ì•ˆ ì„¤ì • ì¶”ê°€
                )
            print(f"âœ… '{doc.title}' ë¬¸ì„œê°€ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ ë° ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ì œì¶œëœ ë¬¸ì„œ ì €ì¥
            self.submitted_docs.append(doc)

            # ìƒìœ„ 3ê°œ ì²­í¬ ì •ë³´
            vector_details = []
            for i in range(min(3, len(documents))):
                vector_details.append({
                    "vector_index": i + 1,
                    "embedding_excerpt": self.vectorstore.index.reconstruct(i)[:5],  # ì„ë² ë”© ì¼ë¶€ ì¶œë ¥
                    "content_excerpt": documents[i].page_content[:300],  # ì²­í¬ ë³¸ë¬¸ ì¼ë¶€ ì¶œë ¥
                    "title": documents[i].metadata["title"],  # ë¬¸ì„œ ì œëª© ì¶”ê°€
                    "url": documents[i].metadata["url"],  # ë¬¸ì„œ URL ì¶”ê°€
                })

            print(vector_details)


            # Save the vectorstore locally
            # self.vectorstore.save_local(self.vectorstore_path)
            # print(f"Vectorstore saved at {self.vectorstore_path}.")
        except Exception as e:
            raise RuntimeError(f"Error processing document: {e}")
    def add_pdf_to_db(self, docs):
        """ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì¶”ê°€"""
        try:
            if not isinstance(docs, list):
                docs = [docs]  # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            documents = []

            for doc in docs:
                splits = text_splitter.split_text(doc.page_content)  # doc.page_content ì‚¬ìš©
                for i, split in enumerate(splits):
                    unique_id = f"{doc.metadata['title']}_{i}"  # titleì„ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ  ID ìƒì„±
                    documents.append(
                        Document(page_content=split, metadata=doc.metadata, id=unique_id)
                    )
                # for split in splits:
                #     documents.append(
                #         Document(page_content=split, metadata=doc.metadata)  # page_content ì‚¬ìš©
                #     )

            # ë²¡í„°ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€
            self.vectorstore.add_documents(documents)
            self.vectorstore.save_local(self.vectorstore_path)

            return {"message": "âœ… ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ë²¡í„° DBì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.", "document_count": len(documents)}

        except Exception as e:
            raise RuntimeError(f"Error processing document: {e}")


    def add_documents(self, documents):
        """Add multiple LangChain Document objects to the vector DB."""
        for doc in documents:
            self.add_doc_to_db(doc) 

    def search(self, query, k, search_type, similarity_threshold):
        """
        Simple search in vector store.
        """
        if not self.vectorstore:
            raise ValueError("Vectorstore is not initialized. Add documents first.")

        retriever = self.get_retriever(search_type, k, similarity_threshold)
        return retriever.invoke(query)

    def get_retriever(self, search_type, k, similarity_threshold):
        """Retrieve documents from the vectorstore."""
        if self.vectorstore is None:
            raise ValueError("Vectorstore is not initialized. Add documents first.")

        return self.vectorstore.as_retriever(search_type=search_type, k=k, similarity_threshold=similarity_threshold)

    def get_submitted_docs(self):
        """Return all submitted documents."""
        return self.submitted_docs
    def get_all_docs_metadata(self):
        """ë²¡í„° DBì— ì €ì¥ëœ ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°(title, url)ë¥¼ ë°˜í™˜"""
        if not self.vectorstore:
            print("âŒ FAISS ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        metadata_list = []
        for doc_id, doc in self.vectorstore.docstore._dict.items():  # docstore ë‚´ë¶€ dict ì ‘ê·¼

            if doc is None:
                continue
            title = doc.metadata.get("title", "ì œëª© ì—†ìŒ")
            url = doc.metadata.get("url", "URL ì—†ìŒ")
            metadata_list.append({"title": title, "url": url})

        return metadata_list
    def get_top_k_vectors(self, k=5):
        """ìƒìœ„ Kê°œì˜ ë²¡í„°ë¥¼ ì¡°íšŒí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            if not self.vectorstore:
                raise RuntimeError("FAISS ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            if len(self.vectorstore.docstore._dict) == 0:
                print("âŒ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []

            # ìƒìœ„ Kê°œì˜ ë¬¸ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê¸°
            documents = list(self.vectorstore.docstore._dict.values())[:k]
            top_k_info = [{"title": doc.metadata.get("title", "N/A"), "content_excerpt": doc.page_content[:300]} for doc in documents]

            return top_k_info

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def delete_doc_by_title(self, title: str):
        """titleì„ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì‚­ì œ"""
        try:
            # ëª¨ë“  ë¬¸ì„œ ì¶œë ¥
            print("ğŸ“„ í˜„ì¬ ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡:")
            for doc_id, doc in self.vectorstore.docstore._dict.items():
                print(f"ID: {doc_id}, Title: {doc.metadata.get('title')}, Metadata: {doc.metadata}")

            # docstoreì—ì„œ titleë¡œ í•´ë‹¹ ID ê°€ì ¸ì˜¤ê¸°
            doc_ids_to_delete = [
                doc_id for doc_id, doc in self.vectorstore.docstore._dict.items()
                if doc.metadata.get("title") == title
            ]

            if not doc_ids_to_delete:
                return {"message": f"âŒ '{title}' ì œëª©ì˜ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

            print(f"ğŸ“ ì‚­ì œí•  ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸: {doc_ids_to_delete}")

            # ì‚­ì œ ìˆ˜í–‰
            self.vectorstore.delete(doc_ids_to_delete)
            self.vectorstore.save_local(self.vectorstore_path)

            return {"message": f"âœ… '{title}' ì œëª©ì˜ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}

        except Exception as e:
            raise RuntimeError(f"Error deleting document by title: {e}")
