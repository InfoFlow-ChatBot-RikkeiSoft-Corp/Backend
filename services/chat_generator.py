from flask import current_app
from langchain.schema import AIMessage, HumanMessage, BaseMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import GoogleGenerativeAI
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from models.models import LLMPrompt  # LLMPrompt ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
from services.prompt import get_default_prompt_template
from operator import itemgetter
from services.chat_service import ChatService
import json
import re

class ChatGenerator:
    def __init__(self, retriever):
        self.llm = GoogleGenerativeAI(model="models/gemini-1.5-flash", temperature=0.7)
        self.retriever = retriever  # ì™¸ë¶€ì—ì„œ ì „ë‹¬ëœ ë²¡í„° ê²€ìƒ‰ retriever
        self.prompt = get_default_prompt_template()

        self.chain = (
            {
                "instruction": itemgetter("instruction"),
                "context": itemgetter("context"), # ë¬¸ì„œ ê²€ìƒ‰ê¸°
                "question": itemgetter("question"),  # ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ëŠ” Runnable
                "chat_history": itemgetter("chat_history"),
            }
            | self.prompt  # í”„ë¡¬í”„íŠ¸ ìƒì„±
            | self.llm # LLM ì‹¤í–‰
            | StrOutputParser()  # ì¶œë ¥ íŒŒì‹±
        )

        self.message_history_store = {}

        # í™œì„±í™”ëœ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° ë° ì„¤ì •
        with current_app.app_context():
            self.prompt_instruction = self.get_prompt_instruction()
        
        self.set_prompt_template()

        # `RunnableWithMessageHistory` ì´ˆê¸°í™”
        self.rag_with_history = RunnableWithMessageHistory(
            runnable=self.chain,
            get_session_history=self.get_session_history,
            input_messages_key="question",
            history_messages_key="chat_history"
        )
        
    def get_prompt_instruction(self):
        """DBì—ì„œ í™œì„±í™”ëœ í”„ë¡¬í”„íŠ¸ ì„¤ëª… ë¶€ë¶„ ê°€ì ¸ì˜¤ê¸°"""
        active_prompt = LLMPrompt.query.filter_by(is_active=True).first()
        if active_prompt:
            print(f"âœ… í™œì„±í™”ëœ í”„ë¡¬í”„íŠ¸ ë¡œë“œë¨: {active_prompt.prompt_name}")
            return active_prompt.prompt_text  # ì„¤ëª… ë¶€ë¶„ë§Œ ë°˜í™˜
        else:
            print("âŒ í™œì„±í™”ëœ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Answer in Korean."

    def set_prompt_template(self):
        """`prompt.py`ì—ì„œ ê°€ì ¸ì˜¨ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì„¤ì •"""
        self.prompt = get_default_prompt_template()

    def get_session_history(self, conversation_id: str) -> ChatMessageHistory:
        if conversation_id not in self.message_history_store:
            print(f"ğŸ”„ ìƒˆ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„±ë¨: {conversation_id}")
            self.message_history_store[conversation_id] = ChatMessageHistory()
        else:
            print(f"âœ… ê¸°ì¡´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°: {conversation_id}")
        return self.message_history_store[conversation_id]

    def add_user_message(self, conversation_id: str, content: str):
        chat_history = self.get_session_history(conversation_id)
        chat_history.add_message(HumanMessage(content=content))
        print(f"âœ… ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ë¨: {content}")  # ë””ë²„ê¹… ë¡œê·¸

    def add_ai_message(self, conversation_id: str, content: str):
        chat_history = self.get_session_history(conversation_id)
        chat_history.add_message(AIMessage(content=content))
        print(f"âœ… AI ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥ë¨: {content}")  # ë””ë²„ê¹… ë¡œê·¸

    # def generate_answer(self, conversation_id, question, context, highest_score_url):
    #     """
    #     ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±.
    #     """
    #     try:
    #         print(f"context: {context}")
    #         # contextê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
    #         if isinstance(context, list):
    #             # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ì—¬ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¦
    #             context_text = "\n\n".join(context)
    #             references = []  # ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ëœ ê²½ìš° ì°¸ì¡° ì •ë³´ë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
    #         elif isinstance(context, dict):
    #             # contextê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
    #             context_text = context.get("context", "ë¬¸ë§¥ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    #             references = context.get("references", [])
    #         else:
    #             # ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì˜ context ì²˜ë¦¬
    #             raise ValueError("`context`ëŠ” ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤.")
            
    #         # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    #         chat_history_object = ChatService.get_recent_chat_history(conversation_id, 10)
    #         chat_history = [history.to_dict() for history in chat_history_object]
            
    #         # ë””ë²„ê¹… ë¡œê·¸
    #         print(f"ğŸ“Š references: {references}")
    #         # print(f"ğŸ“Š Chat History: {chat_history}")

    #         # Invoke LLM with prepared data
    #         input_data = {
    #             "instruction": self.prompt_instruction,
    #             "chat_history": chat_history,
    #             "question": question,
    #             "context": context_text,
    #         }
    #         input_data_str = json.dumps(input_data, indent=4, ensure_ascii=False)
    #         # print(f"ğŸ“ Debug: Input Data for LLM:\n{input_data_str}")

    #         # 4. LLM í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
    #         response = self.llm.invoke(input_data_str)
    #         # ì‘ë‹µ ì²˜ë¦¬
    #         answer = response["output"] if isinstance(response, dict) else response

    #         # ì‚¬ìš©ì ì§ˆë¬¸ ë©”ì‹œì§€ ì¶”ê°€
    #         self.add_user_message(conversation_id, question)
    #         self.add_ai_message(conversation_id, answer)

    #         # ì°¸ì¡° ë¬¸ì„œê°€ ìˆëŠ” ê²½ìš° ì‘ë‹µì— ì¶”ê°€
    #         if references:
    #             reference_texts = "\n".join([f"- {ref['title']} ({ref['url']})" for ref in references])
    #             answer += f"\n\nì°¸ê³  ìë£Œ:\n{reference_texts}"
    #         return answer
    #     except Exception as e:
    #         print(f"âŒ Chain í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    #         return "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def clean_answer(self, answer):
        """
        Removes classification tags like (Casual) or (Informational) and other unnecessary patterns 
        such as 'response classification: ...' from the answer.
        """
        # Remove tags like (Casual) or (Informational)
        answer = re.sub(r'\s*\((Casual|Informational)\)\s*\n*', ' ', answer).strip()
        
        # Remove lines like "response classification: ..."
        answer = re.sub(r'Response Classification: casual', '\n', answer).strip()

        answer = re.sub(r'Response Classification: informational', '\n', answer).strip()
        
        return answer

    def generate_answer(self, conversation_id, question, context, highest_score_url):
        """
        ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ LLMì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±.
        """
        try:
            print(f"context: {context}")
            # contextê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
            if isinstance(context, list):
                # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ì—¬ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¦
                context_text = "\n\n".join(context)
                references = []  # ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬ëœ ê²½ìš°ì˜ ì°¸ì¡° ì •ë³´
            elif isinstance(context, dict):
                # contextê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                context_text = context.get("context", "ë¬¸ë§¥ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                references = context.get("references", [])  # ë”•ì…”ë„ˆë¦¬ì—ì„œ ì°¸ì¡° ì •ë³´ ì¶”ì¶œ
            else:
                # ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì˜ context ì²˜ë¦¬
                raise ValueError("`context`ëŠ” ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤.")
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
            chat_history_object = ChatService.get_recent_chat_history(conversation_id, 10)
            chat_history = [history.to_dict() for history in chat_history_object]
            
            # ë””ë²„ê¹… ë¡œê·¸
            # print(f"ğŸ“Š references: {references}")

            # Invoke LLM with prepared data
            input_data = {
                "instruction": self.prompt_instruction,
                "chat_history": chat_history,
                "question": question,
                "context": context_text,
            }
            input_data_str = json.dumps(input_data, indent=4, ensure_ascii=False)

            # LLM í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
            response = self.llm.invoke(input_data_str)
            # ì‘ë‹µ ì²˜ë¦¬
            answer = response["output"] if isinstance(response, dict) else response
            print(answer)
            answer = self.clean_answer(answer)

            # ì‚¬ìš©ì ì§ˆë¬¸ ë©”ì‹œì§€ ì¶”ê°€
            self.add_user_message(conversation_id, question)
            self.add_ai_message(conversation_id, answer)
            
            return answer
        except Exception as e:
            print(f"âŒ Chain í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
