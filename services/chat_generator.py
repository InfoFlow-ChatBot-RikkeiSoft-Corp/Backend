from flask import current_app
from langchain.schema import AIMessage, HumanMessage, BaseMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import GoogleGenerativeAI
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from models.models import LLMPrompt, db  # LLMPrompt ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°

class ChatGenerator:
    def __init__(self, retriever_manager):
        self.llm = GoogleGenerativeAI(model="models/gemini-1.5-flash", temperature=0.7)
        self.message_history_store = {}
        self.retriever = retriever_manager.vectorstore.as_retriever()

        # í™œì„±í™”ëœ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° ë° ì„¤ì •
        with current_app.app_context():
            self.prompt_instruction = self.get_prompt_instruction()
        # í”„ë¡¬í”„íŠ¸ ë‚´ìš© ì¶œë ¥
        # print(f"ğŸ” í˜„ì¬ í”„ë¡¬í”„íŠ¸ ë‚´ìš©:\n{self.prompt_instruction}\n")
        self.set_prompt_template()

        # `RunnableWithMessageHistory` ì´ˆê¸°í™”
        self.rag_with_history = RunnableWithMessageHistory(
            runnable=self.llm,
            get_session_history=self.get_session_history,
            input_messages_key="chat_history",
            history_messages_key="history"
        )

    def get_prompt_instruction(self):
        """DBì—ì„œ í™œì„±í™”ëœ í”„ë¡¬í”„íŠ¸ ì„¤ëª… ë¶€ë¶„ ê°€ì ¸ì˜¤ê¸°"""
        active_prompt = LLMPrompt.query.filter_by(is_active=True).first()
        if active_prompt:
            print(f"âœ… í™œì„±í™”ëœ í”„ë¡¬í”„íŠ¸ ë¡œë“œë¨: {active_prompt.prompt_name}")
            return active_prompt.prompt_text  # ì„¤ëª… ë¶€ë¶„ë§Œ ë°˜í™˜
        else:
            print("âŒ í™œì„±í™”ëœ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Answer in Korean."

    def set_prompt_template(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •"""
        self.prompt = PromptTemplate.from_template(
            f"""{self.prompt_instruction}

            #Previous Chat History:
            {{chat_history}}

            #Question: 
            {{question}} 

            #Context: 
            {{context}} 

            #Answer:"""
        )
        # print(self.prompt)

    def get_session_history(self, user_id: str) -> ChatMessageHistory:
        if user_id not in self.message_history_store:
            self.message_history_store[user_id] = ChatMessageHistory()
        return self.message_history_store[user_id]

    def add_user_message(self, user_id: str, content: str):
        chat_history = self.get_session_history(user_id)
        chat_history.add_message(HumanMessage(content=content))

    def add_ai_message(self, user_id: str, content: str):
        chat_history = self.get_session_history(user_id)
        chat_history.add_message(AIMessage(content=content))

    def generate_answer(self, user_id, question, context):
        # context êµ¬ì¡°ì—ì„œ ë³¸ë¬¸ê³¼ ì°¸ì¡° ì •ë³´ë¥¼ ë¶„ë¦¬
        context_text = context.get("context", "ë¬¸ë§¥ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        references = context.get("references", [])

        # ì‚¬ìš©ì ì§ˆë¬¸ ë©”ì‹œì§€ ì¶”ê°€
        self.add_user_message(user_id, question)

        # ëŒ€í™” ë‚´ì—­ ê°€ì ¸ì˜¤ê¸°
        chat_history = self.get_session_history(user_id).messages
        # chat_history = "\n".join([message.content for message in self.get_session_history(user_id).messages])
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ë°ì´í„°ë¥¼ ì‚½ì…í•˜ì—¬ ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        formatted_prompt = self.prompt.format(
            chat_history=chat_history,
            question=question,
            context=context_text
        )

        # ë””ë²„ê¹…ìš© ì¶œë ¥
        print(f"ğŸ“ ìµœì¢… í”„ë¡¬í”„íŠ¸:\n{formatted_prompt}")

        # LLM í˜¸ì¶œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        input_messages = [HumanMessage(content=formatted_prompt)]

        # LLM í˜¸ì¶œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        # input_messages = chat_history + [HumanMessage(content=f"ì§ˆë¬¸: {question}\në¬¸ë§¥: {context_text}")]

        try:
            # LLM í˜¸ì¶œ ë° ì‘ë‹µ ìƒì„±
            response = self.llm.invoke(input_messages)
            answer = response.content if isinstance(response, AIMessage) else response

            # ì°¸ì¡° ë¬¸ì„œ ì •ë³´ë¥¼ ë‹µë³€ì— ì¶”ê°€
            if references:
                reference_texts = "\n".join([f"- {ref['title']} ({ref['url']})" for ref in references])
                answer += f"\n\nì°¸ê³  ìë£Œ:\n{reference_texts}"

            # AI ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
            self.add_ai_message(user_id, answer)
            return answer
        except Exception as e:
            print(f"âŒ LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."